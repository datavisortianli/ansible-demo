---
spark_version: "2.1.0"
spark_tar_name: "spark-{{ spark_version }}-bin-hadoop2.6"
spark_mirror: "http://archive.apache.org/dist/spark"
spark_src_dir: "/usr/local/src"
spark_conf_dir: "/etc/spark"
spark_usr_parent_dir: "/opt"  #this is the folder where the spark archive will be extracted
spark_usr_dir: "/opt/spark"   #this is the symlink to the extracted/installed spark
spark_work_dir: "/mnt/spark-worker-tmp"
spark_tmp_dir: "/mnt/spark/tmp"
spark_lib_dir: "/var/lib/spark"
spark_log_dir: "/var/log/spark"
spark_run_dir: "/run/spark"
spark_user: "spark"           # the name of the (OS)user created for spark
spark_user_groups: []         # Optional list of (OS)groups the new spark user should belong to
spark_user_shell: "/bin/false"    # the spark user's default shell

spark_env_extras:
  SPARK_WORKER_INSTANCES: "4"
  SPARK_WORKER_MEMORY: "1g"
  SPARK_DAEMON_WORKER_MEMORY: "1g"
  SPARK_DAEMON_MASTER_MEMORY: "8g"
  SPARK_DAEMON_JAVA_OPTS: "-Dspark.worker.timeout=300 -Dspark.akka.timeout=100 -Dspark.akka.frameSize=100 -Dspark.worker.cleanup.enabled=false -Dspark.worker.cleanup.interval=432000 -Dspark.worker.cle
anup.appDataTtl=432000 -Dspark.shuffle.consolidateFiles=true -Dspark.eventLog.dir=file:///mnt/spark-events -Dspark.eventLog.compress=true -Dspark.ui.retainedJobs=200 -Xms512m -Djava.io.tmpdir=/mnt/java-tmp"
  SPARK_JAVA_OPTS: "-Dspark.worker.timeout=300 -Dspark.akka.timeout=100 -Dspark.akka.frameSize=100 -Dspark.worker.cleanup.enabled=false -Dspark.worker.cleanup.interval=432000 -Dspark.worker.cleanup.ap
pDataTtl=432000 -Dspark.shuffle.consolidateFiles=true -Dspark.eventLog.dir=file:///mnt/spark-events -Dspark.eventLog.compress=true -Dspark.ui.retainedJobs=200 -Xms512m -Djava.io.tmpdir=/mnt/java-tmp"

spark_defaults_extras: {}
